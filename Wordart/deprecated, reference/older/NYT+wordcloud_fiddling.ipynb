{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2, json, nltk\n",
    "import math, random, time, pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(page):\n",
    "    url_base = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?'\n",
    "    url_date = '&begin_date=19850814&end_date=19850816'\n",
    "    url_key = '&api-key=6940176db80043d5a17343d873cfc82e'\n",
    "    url_page = '&page=' + str(page)\n",
    "    if page>100: raise NameError('Cannot request beyond page 100')\n",
    "    url_assembled = url_base + url_date + url_page + url_key\n",
    "    response = urllib2.urlopen(url_assembled)\n",
    "    json_data = response.read()\n",
    "    data = json.loads(json_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def getNouns(data, noun_list):\n",
    "    for i in range(0,9):\n",
    "        sentence = data['response']['docs'][i]['lead_paragraph']\n",
    "        if not(sentence == ''or sentence is None):\n",
    "            text = nltk.word_tokenize(sentence)\n",
    "            poses = nltk.pos_tag(text)\n",
    "            prev_noun_flag = False;\n",
    "            for item in poses:\n",
    "                if item[1] == 'NNP':\n",
    "                    if prev_noun_flag:\n",
    "                        noun_list[-1]=noun_list[-1] + ' ' + item[0]\n",
    "                    else:\n",
    "                        noun_list.append(item[0])\n",
    "                    prev_noun_flag = True\n",
    "                else: prev_noun_flag = False\n",
    "            return noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 pages found\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lead_paragraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-98616a639ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgetNouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoun_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remember lists are mutable and python passes by reference!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#time.sleep(0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e4765ab6cb39>\u001b[0m in \u001b[0;36mgetNouns\u001b[0;34m(data, noun_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetNouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoun_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lead_paragraph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lead_paragraph'"
     ]
    }
   ],
   "source": [
    "data = getPage(0)\n",
    "num_pages = int(math.floor(data['response']['meta']['hits']/10))-1\n",
    "print str(num_pages)+' pages found'\n",
    "noun_list = []\n",
    "for page in range(0,num_pages):\n",
    "    data = getPage(page)\n",
    "    getNouns(data, noun_list) #remember lists are mutable and python passes by reference!\n",
    "    print str(page),\n",
    "    #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677 proper nouns/phrases found, such as:\n",
      "Atlantic\n",
      "Coca-Cola\n",
      "House Foreign\n",
      "Senator Gary Hart\n",
      "Presley\n",
      "South Street Seaport\n",
      "John Walker\n",
      "M-S-R Public Power Agency\n",
      "Steve\n"
     ]
    }
   ],
   "source": [
    "print str(len(noun_list))+' proper nouns/phrases found, such as:'\n",
    "for count in range(1,10):\n",
    "    print noun_list[random.randrange(0, len(noun_list), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list_no_spaces = list(noun_list)\n",
    "noun_list_no_spaces[:]=[item.encode('ascii').replace(' ','') for item in noun_list_no_spaces] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_of_nouns = ' '.join(noun_list_no_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE OUT THE DATA SO WE DON'T HAVE TO KEEP RE-POLLING\n",
    "import pickle\n",
    "outfile = open('string_nouns.txt','w')\n",
    "pickle.dump(string_of_nouns,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD BACK IN DATA\n",
    "import pickle\n",
    "outfile = open('string_nouns.txt','r')\n",
    "string_of_nouns = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pil way (if you don't have matplotlib)\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from os import path\n",
    "\n",
    "\n",
    "leo_mask = np.array(Image.open(\"leo_paint_color.png\"))\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=leo_mask, max_font_size=24)\n",
    "wc.generate(string_of_nouns)\n",
    "\n",
    "#recolor\n",
    "image_colors = ImageColorGenerator(leo_mask)\n",
    "wc.recolor(color_func=image_colors)\n",
    "\n",
    "#save\n",
    "wc.to_file(\"leo_out.png\")\n",
    "\n",
    "#show\n",
    "image = wc.to_image()\n",
    "image.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO Fix wrongly detected nouns\n",
    "\n",
    "#TODO why freezing with matplotlib??\n",
    "\n",
    "        # Display the generated image:\n",
    "        # the matplotlib way:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # take relative word frequencies into account, lower max_font_size\n",
    "        wordcloud = WordCloud(max_font_size=40, relative_scaling=.5).generate(string_of_nouns)\n",
    "        plt.figure()\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "\n",
    "        \n",
    "        ### NOTES\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger') # need 'punkt','averaged_perceptron_tagger'\n",
    "#from __future__ import unicode_literals #??\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
